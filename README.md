# ğŸ“š RAG-ChatBot

A Retrieval-Augmented Generation (RAG) based chatbot that combines the power of Large Language Models (LLMs) with document-based retrieval. The system allows context-aware answering by retrieving relevant chunks from external documents and passing them to the LLM for response generation.

---

## ğŸ”§ Features

- ğŸ“„ Ingests and indexes documents  
- ğŸ” Contextual retrieval of information using semantic similarity  
- ğŸ¤– Uses a local or remote LLM (like OpenAI or HuggingFace)  
- ğŸ§  Combines retrieval results with prompt for better answers  
- ğŸ”„ Modular and extendable codebase  

---

## ğŸ—‚ï¸ Folder Structure

```
RAG-ChatBot/
â”œâ”€â”€ app.py                 # Main Streamlit app
â”œâ”€â”€ loader.py              # Loads and chunks documents
â”œâ”€â”€ rag_chain.py           # Defines the RAG chain logic
â”œâ”€â”€ environment.yml        # Conda environment setup
â”œâ”€â”€ requirements.txt       # Python package dependencies
â”œâ”€â”€ .env                   # Environment variables (keys, paths)
â””â”€â”€ README.md              # You're here
```

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/BenTennyson100/RAG-ChatBot.git
cd RAG-ChatBot
```

### 2. Set Up Environment

Using Conda:

```bash
conda env create -f environment.yml
conda activate rag-chatbot
```

Or using pip:

```bash
pip install -r requirements.txt
```

### 3. Add Environment Variables

Create a `.env` file and add:

```env
OPENAI_API_KEY=your_openai_key
```

*(or any other LLM provider's key you're using)*

### 4. Run the App

```bash
streamlit run app.py
```

---

## ğŸ§© Tech Stack

- Python  
- LangChain  
- Streamlit  
- OpenAI / HuggingFace  
- FAISS / Chroma (Vector DBs)

---

## ğŸ› ï¸ To Do

- Add PDF/website/document upload via UI  
- Multi-turn conversation memory  
- Deploy on HuggingFace Spaces or Streamlit Cloud  

---

## ğŸ¤ Contributions

Pull requests and suggestions are welcome!

---

## ğŸ“„ License

[MIT License](LICENSE)
